{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree, svm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"star_classification_Prep.csv\")\n",
    "X = data1.drop(\"class\", axis = 1)\n",
    "y = data1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b997b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b55534",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31078af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing diffrent test sizes to see which one has the best preformance. \n",
    "# We will be looking at values between .1 and .5 taking step sizes of .05\n",
    "\n",
    "test_accuracy_score = [] \n",
    "train_accuracy_score = []\n",
    "for i in np.linspace(.1,.5,9):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=42)\n",
    "    classifier = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    test_accuracy_score.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    train_accuracy_score.append(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting average test and training accuracys for diffrent test sizes\n",
    "\n",
    "X_axis =  np.linspace(.1,.5,9)\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, test_accuracy_score, label = 'Test Accuracy')\n",
    "plt.plot(X_axis, train_accuracy_score, label = 'Train Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Test Size\")\n",
    "plt.ylabel(\"Accuracy Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=X_axis[\n",
    "    np.argmax(test_accuracy_score)], random_state=42)\n",
    "\n",
    "print(\"Best test size: \", X_axis[np.argmax(test_accuracy_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ca5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3a86b4",
   "metadata": {},
   "source": [
    "***Hyperparameters to be tested***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96704d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(knn, knn_param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "# Extract the mean cross-validation scores and hyperparameters\n",
    "mean_scores = results['mean_test_score']\n",
    "params = results['params']\n",
    "\n",
    "# Convert hyperparameters to strings for plotting\n",
    "param_strings = [str(param) for param in params]\n",
    "\n",
    "# Plot the cross-validation scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(np.arange(len(param_strings)), mean_scores, align='center')\n",
    "plt.yticks(np.arange(len(param_strings)), param_strings)\n",
    "plt.xlabel('Mean Train Score')\n",
    "plt.ylabel('Hyperparameters')\n",
    "plt.title('Results')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display top-to-bottom\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "print(\"Test set accuracy with best parameters:\", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcb13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f06570a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing diffrent test sizes to see which one has the best preformance. \n",
    "# We will be looking at values between .1 and .5 taking step sizes of .05\n",
    "\n",
    "test_accuracy_score = [] \n",
    "train_accuracy_score = []\n",
    "for i in np.linspace(.1,.5,9):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=42)\n",
    "    classifier = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    test_accuracy_score.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    train_accuracy_score.append(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25306f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting average test and training accuracys for diffrent test sizes\n",
    "\n",
    "X_axis =  np.linspace(.1,.5,9)\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, test_accuracy_score, label = 'Test Accuracy')\n",
    "plt.plot(X_axis, train_accuracy_score, label = 'Train Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs. Test Size')\n",
    "plt.xlabel(\"Test Size\")\n",
    "plt.ylabel(\"Accuracy Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=X_axis[\n",
    "    np.argmax(test_accuracy_score)], random_state=42)\n",
    "\n",
    "print(\"Best test size: \", X_axis[np.argmax(test_accuracy_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5978e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ed692c9",
   "metadata": {},
   "source": [
    "***Hyperparameters to be tested***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(), tree_param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Plotting accuracy vs. hyperparameters\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting mean training score\n",
    "sns.lineplot(data=results, x='param_max_depth', y='mean_test_score', hue='param_criterion', marker='o', palette='viridis')\n",
    "plt.title('Mean Train Score vs. Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Mean Train Score')\n",
    "plt.legend(title='Criterion')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ff35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "print(\"Test accuracy with best parameters:\", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80608f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dade8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a08b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(**grid_search.best_params_)\n",
    "path1 = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "alphas1, impurities1 = path1.ccp_alphas, path1.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test each alpha value to see which give the best accuracy.\n",
    "# The data will be split dependent on the best scoring test size from the previous Experiment\n",
    "\n",
    "clfs1 = []\n",
    "\n",
    "for ccp_alpha in tqdm(alphas1):\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs1.append(clf)\n",
    "    \n",
    "scores = [clf.score(X_train, y_train) for clf in tqdm(clfs1)]\n",
    "best_alpha = alphas1[np.argmax(scores)]\n",
    "print(\"Best alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_scores1 = [accuracy_score(y_test, clf.predict(X_test)) for clf in tqdm(clfs1)]\n",
    "train_acc_scores1 = [accuracy_score(y_train, clf.predict(X_train)) for clf in tqdm(clfs1)]\n",
    "\n",
    "tree_depths = [clf.tree_.max_depth for clf in clfs1]\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(alphas1[:-1], test_acc_scores1[:-1], label=\"Test Scores\")\n",
    "plt.plot(alphas1[:-1], train_acc_scores1[:-1], label = \"Train Scores\")\n",
    "plt.xlabel(\"Effective alpha\")\n",
    "plt.ylabel(\"Accuracy scores\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5385638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bec74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85728530",
   "metadata": {},
   "source": [
    "# Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3317374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing diffrent test sizes to see which one has the best preformance. \n",
    "# We will be looking at values between .1 and .5 taking step sizes of .05\n",
    "\n",
    "test_accuracy_score = [] \n",
    "train_accuracy_score = []\n",
    "for i in np.linspace(.1,.5,9):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=42)\n",
    "    classifier = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    test_accuracy_score.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    train_accuracy_score.append(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdae531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting average test and training accuracys for diffrent test sizes\n",
    "\n",
    "X_axis =  np.linspace(.1,.5,9)\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, test_accuracy_score, label = 'Test Accuracy')\n",
    "plt.plot(X_axis, train_accuracy_score, label = 'Train Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs. Test Size')\n",
    "plt.xlabel(\"Test Size\")\n",
    "plt.ylabel(\"Accuracy Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=X_axis[\n",
    "    np.argmax(test_accuracy_score)], random_state=42)\n",
    "\n",
    "print(\"Best test size: \", X_axis[np.argmax(test_accuracy_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bd54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "821479ed",
   "metadata": {},
   "source": [
    "***Hyperparameters to be tested***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c55fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3432bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(GradientBoostingClassifier(), bdt_param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f51133",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "parameters_to_plot = ['learning_rate', 'n_estimators', 'max_depth']\n",
    "\n",
    "# Create subplots for each parameter\n",
    "fig, axes = plt.subplots(len(parameters_to_plot), 1, figsize=(10, 6*len(parameters_to_plot)))\n",
    "for i, param in enumerate(parameters_to_plot):\n",
    "    ax = axes[i] if len(parameters_to_plot) > 1 else axes\n",
    "    sns.lineplot(data=results, x=param, y='mean_test_score', marker='o', ax=ax)\n",
    "    ax.set_title(f'Test Scores vs. {param}')\n",
    "    ax.set_xlabel(param.split('_')[-1].capitalize())  # Extract parameter name\n",
    "    ax.set_ylabel('Test Score')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508eaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "print(\"Test accuracy with best parameters:\", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "# Extract the mean cross-validation scores and hyperparameters\n",
    "mean_scores = results['mean_test_score']\n",
    "params = results['params']\n",
    "\n",
    "# Convert hyperparameters to strings for plotting\n",
    "param_strings = [str(param) for param in params]\n",
    "\n",
    "# Plot the cross-validation scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(np.arange(len(param_strings)), mean_scores, align='center')\n",
    "plt.yticks(np.arange(len(param_strings)), param_strings)\n",
    "plt.xlabel('Mean Train Score')\n",
    "plt.ylabel('Hyperparameters')\n",
    "plt.title('Results')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display top-to-bottom\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e201e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d015b7d",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing diffrent test sizes to see which one has the best preformance. \n",
    "# We will be looking at values between .1 and .5 taking step sizes of .05\n",
    "\n",
    "test_accuracy_score = [] \n",
    "train_accuracy_score = []\n",
    "for i in np.linspace(.1,.5,9):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=42)\n",
    "    classifier = svm.SVC().fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    test_accuracy_score.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    train_accuracy_score.append(accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25933892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting average test and training accuracys for diffrent test sizes\n",
    "\n",
    "X_axis =  np.linspace(.1,.5,9)\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, test_accuracy_score, label = 'Test Accuracy')\n",
    "plt.plot(X_axis, train_accuracy_score, label = 'Train Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs. Test Size')\n",
    "plt.xlabel(\"Test Size\")\n",
    "plt.ylabel(\"Accuracy Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb52bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=X_axis[\n",
    "    np.argmax(test_accuracy_score)], random_state=42)\n",
    "\n",
    "print(\"Best test size: \", X_axis[np.argmax(test_accuracy_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf08bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6393ae18",
   "metadata": {},
   "source": [
    "***Hyperparameters to be tested***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4218bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(svm.SVC(), svm_param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68843c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "\n",
    "# Extract parameter values and scores\n",
    "params = results['params']\n",
    "mean_test_scores = results['mean_test_score']\n",
    "\n",
    "# Extract parameter names for plotting\n",
    "param_names = list(params[0].keys())\n",
    "\n",
    "# Fix other parameters and plot mean test score against the parameter of interest\n",
    "for param_name in param_names:\n",
    "    # Fix other parameters and their values\n",
    "    fixed_params = {p: v for p, v in params[0].items() if p != param_name}\n",
    "    \n",
    "    # Group mean test scores by the parameter of interest\n",
    "    grouped_scores = {}\n",
    "    for param, score in zip(params, mean_test_scores):\n",
    "        key = param[param_name]\n",
    "        if key not in grouped_scores:\n",
    "            grouped_scores[key] = []\n",
    "        grouped_scores[key].append(score)\n",
    "    \n",
    "    # Plot mean test score against the parameter of interest\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for key, scores in grouped_scores.items():\n",
    "        plt.plot([key] * len(scores), scores, 'o', label=f'{param_name}={key}')\n",
    "    plt.title(f'Mean Test Score for Different Values of {param_name}')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Mean Test Score (Accuracy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "print(\"Test accuracy with best parameters:\", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99087390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56242585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9d244f4",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720db6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values.tolist(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values.tolist(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values.tolist(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values.tolist(), dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid.values.tolist(), dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.values.tolist(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb423acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634568d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_size, hidden_sizes, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = max(y_train)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104546f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for train and test data\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_acc(valid_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in valid_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbaa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    avg_loss = 0\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (data, targets) in enumerate(train_data):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        avg_loss += int(loss)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return avg_loss/(batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7c2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58372f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_tloss = []\n",
    "total_vacc = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "hidden_sizes = [[32, 16, 8], [128, 64], [512, 256, 128, 64], [128, 256, 512, 256, 128, 64]]\n",
    "\n",
    "models = []\n",
    "for i in range(len(hidden_sizes)):\n",
    "    models.append(NeuralNetwork(input_size, hidden_sizes[i], output_size))\n",
    "\n",
    "for model in models:\n",
    "    vacc = []\n",
    "    tloss = []\n",
    "    # Training loop\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 10\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        tloss.append(train_model(train_loader))\n",
    "        vacc.append(valid_acc(valid_loader))\n",
    "        \n",
    "    total_vacc.append(vacc)\n",
    "    total_tloss.append(tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training loss for diffrent hidden sizes\n",
    "\n",
    "X_axis =  [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, total_tloss[0], label = 'model 1')\n",
    "plt.plot(X_axis, total_tloss[1], label = 'model 2')\n",
    "plt.plot(X_axis, total_tloss[2], label = 'model 3')\n",
    "plt.plot(X_axis, total_tloss[3], label = 'model 4')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc76c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the validation acc for diffrent hidden sizes\n",
    "\n",
    "X_axis =  [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, total_vacc[0], label = 'model 1')\n",
    "plt.plot(X_axis, total_vacc[1], label = 'model 2')\n",
    "plt.plot(X_axis, total_vacc[2], label = 'model 3')\n",
    "plt.plot(X_axis, total_vacc[3], label = 'model 4')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca740b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = 0\n",
    "hidden_pos = 0\n",
    "for i in range(len(total_vacc)):\n",
    "    if max(total_vacc[i]) > vmax:\n",
    "        vmax = max(total_vacc[i]) \n",
    "        hidden_pos = i\n",
    "        \n",
    "print(f\"model {hidden_pos+1} had the highest valid acc at {vmax} with layers {hidden_sizes[hidden_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e192aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[hidden_pos]\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        outputs = best_model(data)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68776040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174a95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cf142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = [5, 10, 50, 100]\n",
    "total_tloss = []\n",
    "total_vacc = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "models = []\n",
    "for i in range(len(epochs)):\n",
    "    models.append(NeuralNetwork(input_size, hidden_sizes[hidden_pos], output_size))\n",
    "    \n",
    "for i in range(len(epochs)):\n",
    "    model = models[i]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    tloss = []\n",
    "    vacc = []\n",
    "    for epoch in tqdm(range(epochs[i])):\n",
    "        tloss.append(train_model(train_loader))\n",
    "        vacc.append(valid_acc(valid_loader))\n",
    "        \n",
    "    total_vacc.append(vacc)\n",
    "    total_tloss.append(tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = 0\n",
    "epoch_pos = 0\n",
    "for i in range(len(total_vacc)):\n",
    "    if max(total_vacc[i]) > vmax:\n",
    "        vmax = max(total_vacc[i]) \n",
    "        epoch_pos = i\n",
    "        \n",
    "print(f\"the best epoch size is {epochs[epoch_pos]} with the highest valid acc at {vmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d56791",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis =  list(range(1, epochs[epoch_pos]+1))\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, total_tloss[epoch_pos], label = 'train loss')\n",
    "plt.plot(X_axis, total_vacc[epoch_pos], label = 'valid acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9065ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[epoch_pos]\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        outputs = best_model(data)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a60f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d362326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae54f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lrs = [.1, .01, .005, .001]\n",
    "total_tloss = []\n",
    "total_vacc = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "models = []\n",
    "for i in range(len(lrs)):\n",
    "    models.append(NeuralNetwork(input_size, hidden_sizes[hidden_pos], output_size))\n",
    "    \n",
    "for i in range(len(lrs)):\n",
    "    model = models[i]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lrs[i])\n",
    "    tloss = []\n",
    "    vacc = []\n",
    "    for epoch in tqdm(range(epochs[epoch_pos])):\n",
    "        tloss.append(train_model(train_loader))\n",
    "        vacc.append(valid_acc(valid_loader))\n",
    "        \n",
    "    total_vacc.append(vacc)\n",
    "    total_tloss.append(tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = 0\n",
    "lr_pos = 0\n",
    "for i in range(len(total_vacc)):\n",
    "    if max(total_vacc[i]) > vmax:\n",
    "        vmax = max(total_vacc[i]) \n",
    "        lr_pos = i\n",
    "        \n",
    "print(f\"learning rate of {lrs[lr_pos]} had the highest valid acc at {vmax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting average test and training accuracys for diffrent test sizes\n",
    "\n",
    "X_axis = list(range(1, epochs[epoch_pos]+1))\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, total_tloss[0], label = '.1')\n",
    "plt.plot(X_axis, total_tloss[1], label = '.01')\n",
    "plt.plot(X_axis, total_tloss[2], label = '.005')\n",
    "plt.plot(X_axis, total_tloss[3], label = '.001')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the validation acc for diffrent hidden sizes\n",
    "\n",
    "X_axis =  list(range(1, epochs[epoch_pos]+1))\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(X_axis, total_vacc[0], label = '.1')\n",
    "plt.plot(X_axis, total_vacc[1], label = '.01')\n",
    "plt.plot(X_axis, total_vacc[2], label = '.005')\n",
    "plt.plot(X_axis, total_vacc[3], label = '.001')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[lr_pos]\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        outputs = best_model(data)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
